\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{geometry}
\usepackage{float}
\usepackage[backend=biber,style=numeric]{biblatex}
\addbibresource{./ref.bib}

\geometry{
    a4paper,
    left=30mm,
    right=25mm,
    top=25mm,
    bottom=25mm
}

\title{PANORMA: Pancreatis Cancer Diagnosis: Radiologist Meets AI}
% add subtitle

\author{Adem Kaya, George Lalidis, and Tam Van}

\date{May 2025}

\begin{document}

\maketitle

\section{Introduction}

\section{Methodology}

\subsection{Data Augmentation}
\subsubsection{Data Balancing}
\subsubsection{Data Alteration}
%\subsubsection{Data Masking}

\subsection{Custom Loss Function}
We implemented a custom compound loss that takes a weighted combination of the Dice loss and the Cross-Entropy 
(CE) loss. Dice loss is particularly popular for image segmentation tasks due to its robust nature even when applied
imbalanced datasets. 

CE is another commonly used metric that is used as a loss function for deep learning tasks. CE measures the difference
measures the difference between two probability distributions for some random variable and can be formally denoted:

\begin{equation}
    \centering
    ce = -\sum{}
\end{equation}
\section{Results}
\section{Discussion}
\printbibliography

\end{document}
