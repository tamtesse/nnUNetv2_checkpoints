\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{geometry}
\usepackage{float}
\usepackage[backend=biber,style=numeric]{biblatex}
\newcommand{\curly}{\mathrel{\leadsto}}

\addbibresource{./ref.bib}

\geometry{
    a4paper,
    left=30mm,
    right=25mm,
    top=25mm,
    bottom=25mm
}

\title{PANORMA: Pancreatis Cancer Diagnosis: Radiologist Meets AI}
% add subtitle

\author{Adem Kaya, George Lalidis, and Tam Van}

\date{May 2025}

\begin{document}

\maketitle

\section{Introduction}

\section{Methodology}

\subsection{Data Augmentation}
\subsubsection{Data Balancing}
\subsubsection{Data Alteration}
%\subsubsection{Data Masking}

\subsection{Custom Loss Functions}
\subsubsection{Dice and CE}
We implemented a custom compound loss that takes a weighted combination of the Dice loss and the Cross-Entropy 
(CE) loss. Dice loss is particularly popular for image segmentation tasks due to its robust nature even when applied
imbalanced datasets. 

CE is another commonly used metric that is used as a loss function for deep learning tasks. CE measures the difference
measures the difference between two probability distributions for some random variable and can be formally denoted:

\begin{equation}
\mathcal{L}_{\mathrm{CrossEntropy}}(\mathbf{Y}, \hat{\mathbf{Y}}) = -w_{y_n} \log\frac{\exp(\mathbf{y}_{n, \hat{\mathbf{y}}})}{\sum_{c=1}^{C} \exp(\mathbf{y}_{n,c})} \cdot \hat{\mathbf{y}}_n,
\end{equation}
% https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html
where 

\begin{align*}
    \mathbf{Y}          &: \text{model output}, \\
    \hat{\mathbf{Y}}    &: \text{target label}, \\
    C                   &: \text{all different classes}, \\
    w_{\mathbf{y}_n}    &: \text{weight}.
\end{align*}

The Dice loss is defined as following:

\begin{equation}
\mathcal{L}_{\mathrm{Dice}} = \frac{2 \cdot tp}{2 \cdot tp + fp + fn},
\end{equation}

where

\begin{align*}
    tp          &: \text{True Positives}, \\
    fp          &: \text{False Positives}, \\
    fn          &: \text{False Negatives}.
\end{align*}

\citeauthor{customloss} have shown that both the CE loss as well as the CE loss can be leveraged to achieve better segmentation
results by providing the model with a compound of the two. Where the baseline model has provided a compound of these two losses
with static weights, \citeauthor{customloss} have proposed two compounds they refer to as either soft fine-tuning or hard fine-tuning.
For the soft fine-tuning, the weights linearly transfer from a full weight for the CE loss to full weigh for the Dice loss as the 
epochs progress. Formally, we can express this soft fine tuning as:

\begin{equation}
\mathcal{L}_{\mathrm{BCE \curly Dice}}(\mathbf{Y}, \hat{\mathbf{Y}}) = \frac{N-n}{N} \cdot \mathcal{L}_{BCE}(\mathbf{Y}, \hat{\mathbf{Y}}) + \frac{n}{N} \cdot \mathcal{L}_{Dice}(\mathbf{Y}, \hat{\mathbf{Y}}),
\end{equation}

Even though this loss function performs exceptionally well for in distribution data, \citeauthor{customloss} found that for
out of distrubution data a variation they refer to as hard fine-tunign yields higher performance. For this compound, the
output of the loss function is either the CE loss or the Dice loss, such that it uses the CE loss for the first 90\% of epochs,
and Dice loss for the remaining 10\%. Formally, we can write the hard fine-tuning as:

\begin{equation}
\mathcal{L}_{\text{BCE} \rightarrow \text{Dice}}(\mathbf{y}, \hat{\mathbf{y}}) =
\begin{cases}
\mathcal{L}_{\text{BCE}}(\mathbf{y}, \hat{\mathbf{y}}), & \text{if } n < 0.9 \cdot N \\
\mathcal{L}_{\text{Dice}}(\mathbf{y}, \hat{\mathbf{y}}), & \text{otherwise}
\end{cases}
\end{equation}

\subsubsection{Dice and Focal}
\citeauthor{MA2021102035} have shown that a compound loss, featuring a simple unweighted sum of the Dice as well as the 
Focal loss, excells in performance in a range of 19 different (compound-) losses, for the task of pancreas segmentation.
Therefore, we have implemented this loss function a well. We use the same dice loss as in %\ref{eq:dice}
For the focal loss, we use the following equation:


\section{Results}
\section{Discussion}
\printbibliography

\end{document}
